{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Packages and Dependencies\n",
    "\n",
    "```\n",
    "conda create -n interpretabnet python=3.10\n",
    "conda activate interpretabnet\n",
    "```\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import wget\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "Create a directory named \"data\" within InterpreTabNet.\n",
    "\n",
    "```\n",
    "mkdir data\n",
    "```\n",
    "\n",
    "Download and place your desired dataset into /InterpreTabNet/data.\n",
    "\n",
    "Modify the below according to the comments to process your dataset. The following is an example for the \"Adult\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_973557/3176484843.py:38: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'wealthy' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train.loc[train[target] == 0, target] = \"wealthy\"\n"
     ]
    }
   ],
   "source": [
    "dataset = 'census-income' # Modify Dataset Name\n",
    "\n",
    "out = Path(os.getcwd() + '/data/' + dataset + '.csv')\n",
    "train = pd.read_csv(out)\n",
    "\n",
    "target = ' <=50K' # Modify Target Name\n",
    "\n",
    "if \"Set\" not in train.columns:\n",
    "    train[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p=[.8, .1, .1], size=(train.shape[0],))\n",
    "\n",
    "train_indices = train[train.Set == \"train\"].index\n",
    "valid_indices = train[train.Set == \"valid\"].index\n",
    "test_indices = train[train.Set == \"test\"].index\n",
    "\n",
    "nunique = train.nunique()\n",
    "types = train.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims = {}\n",
    "for col in train.columns:\n",
    "    if types[col] == 'object' or nunique[col] < 200:\n",
    "        # print(col, train[col].nunique())\n",
    "        l_enc = LabelEncoder()\n",
    "        train[col] = train[col].fillna(\"VV_likely\")\n",
    "        train[col] = l_enc.fit_transform(train[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)\n",
    "    else:\n",
    "        train.fillna(train.loc[train_indices, col].mean(), inplace=True)\n",
    "\n",
    "# Setting the target as a categorical feature\n",
    "train.loc[train[target] == 0, target] = \"wealthy\"\n",
    "train.loc[train[target] == 1, target] = \"not_wealthy\"\n",
    "\n",
    "unused_feat = ['Set']\n",
    "\n",
    "features = [col for col in train.columns if col not in unused_feat + [target]]\n",
    "\n",
    "cat_idxs = [i for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "X_train = train[features].values[train_indices]\n",
    "y_train = train[target].values[train_indices]\n",
    "\n",
    "X_valid = train[features].values[valid_indices]\n",
    "y_valid = train[target].values[valid_indices]\n",
    "\n",
    "X_test = train[features].values[test_indices]\n",
    "y_test = train[target].values[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used cuda: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.37305 | train_accuracy: 0.9229  | valid_accuracy: 0.92986 |  0:00:04s\n",
      "epoch 1  | loss: 0.08623 | train_accuracy: 0.9162  | valid_accuracy: 0.9234  |  0:00:07s\n",
      "epoch 2  | loss: 0.03123 | train_accuracy: 0.9199  | valid_accuracy: 0.93124 |  0:00:11s\n",
      "epoch 3  | loss: 0.01039 | train_accuracy: 0.91736 | valid_accuracy: 0.9317  |  0:00:15s\n",
      "epoch 4  | loss: 0.00388 | train_accuracy: 0.91898 | valid_accuracy: 0.93309 |  0:00:19s\n",
      "epoch 5  | loss: 0.0008  | train_accuracy: 0.92982 | valid_accuracy: 0.94186 |  0:00:23s\n",
      "epoch 6  | loss: -0.00032| train_accuracy: 0.93813 | valid_accuracy: 0.95155 |  0:00:26s\n",
      "epoch 7  | loss: -0.00089| train_accuracy: 0.94922 | valid_accuracy: 0.95754 |  0:00:30s\n",
      "epoch 8  | loss: -0.00062| train_accuracy: 0.97045 | valid_accuracy: 0.97508 |  0:00:34s\n",
      "epoch 9  | loss: -0.00133| train_accuracy: 0.98453 | valid_accuracy: 0.98662 |  0:00:38s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.98662\n",
      "Best weights from best epoch are automatically used!\n",
      "Optimum Hyperparameters Training [16, 3, 1.5, 0.025]\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.34868 | train_accuracy: 0.75139 | valid_accuracy: 0.73143 |  0:00:03s\n",
      "epoch 1  | loss: 0.03794 | train_accuracy: 0.92613 | valid_accuracy: 0.92247 |  0:00:07s\n",
      "epoch 2  | loss: 0.00514 | train_accuracy: 0.94737 | valid_accuracy: 0.94739 |  0:00:11s\n",
      "epoch 3  | loss: 2e-05   | train_accuracy: 0.94668 | valid_accuracy: 0.95247 |  0:00:15s\n",
      "epoch 4  | loss: -0.00095| train_accuracy: 0.95129 | valid_accuracy: 0.95062 |  0:00:19s\n",
      "epoch 5  | loss: -0.00141| train_accuracy: 0.9603  | valid_accuracy: 0.95708 |  0:00:22s\n",
      "epoch 6  | loss: -0.00166| train_accuracy: 0.97715 | valid_accuracy: 0.97923 |  0:00:26s\n",
      "epoch 7  | loss: -0.00137| train_accuracy: 0.97946 | valid_accuracy: 0.97923 |  0:00:30s\n",
      "epoch 8  | loss: -0.00143| train_accuracy: 0.98684 | valid_accuracy: 0.98985 |  0:00:34s\n",
      "epoch 9  | loss: -0.00166| train_accuracy: 0.99446 | valid_accuracy: 0.99631 |  0:00:38s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.99631\n",
      "Best weights from best epoch are automatically used!\n",
      "Optimum Hyperparameters Training [32, 3, 1.5, 0.025]\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.16118 | train_accuracy: 0.94114 | valid_accuracy: 0.93816 |  0:00:03s\n",
      "epoch 1  | loss: 0.0066  | train_accuracy: 0.97715 | valid_accuracy: 0.97277 |  0:00:07s\n",
      "epoch 2  | loss: 0.00497 | train_accuracy: 0.98153 | valid_accuracy: 0.98016 |  0:00:11s\n",
      "epoch 3  | loss: -0.0015 | train_accuracy: 0.98638 | valid_accuracy: 0.98431 |  0:00:15s\n",
      "epoch 4  | loss: -0.00179| train_accuracy: 0.98753 | valid_accuracy: 0.98662 |  0:00:19s\n",
      "epoch 5  | loss: -0.00187| train_accuracy: 0.98846 | valid_accuracy: 0.988   |  0:00:23s\n",
      "epoch 6  | loss: -0.00193| train_accuracy: 0.98961 | valid_accuracy: 0.98985 |  0:00:27s\n",
      "epoch 7  | loss: -0.00192| train_accuracy: 0.991   | valid_accuracy: 0.99123 |  0:00:30s\n",
      "epoch 8  | loss: -0.00193| train_accuracy: 0.99146 | valid_accuracy: 0.99123 |  0:00:34s\n",
      "epoch 9  | loss: -0.00191| train_accuracy: 0.99192 | valid_accuracy: 0.99308 |  0:00:38s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.99308\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.34868 | train_accuracy: 0.75139 | valid_accuracy: 0.73143 |  0:00:03s\n",
      "epoch 1  | loss: 0.03794 | train_accuracy: 0.92613 | valid_accuracy: 0.92247 |  0:00:07s\n",
      "epoch 2  | loss: 0.00514 | train_accuracy: 0.94737 | valid_accuracy: 0.94739 |  0:00:11s\n",
      "epoch 3  | loss: 2e-05   | train_accuracy: 0.94668 | valid_accuracy: 0.95247 |  0:00:15s\n",
      "epoch 4  | loss: -0.00095| train_accuracy: 0.95129 | valid_accuracy: 0.95062 |  0:00:19s\n",
      "epoch 5  | loss: -0.00141| train_accuracy: 0.9603  | valid_accuracy: 0.95708 |  0:00:23s\n",
      "epoch 6  | loss: -0.00166| train_accuracy: 0.97715 | valid_accuracy: 0.97923 |  0:00:27s\n",
      "epoch 7  | loss: -0.00137| train_accuracy: 0.97946 | valid_accuracy: 0.97923 |  0:00:30s\n",
      "epoch 8  | loss: -0.00143| train_accuracy: 0.98684 | valid_accuracy: 0.98985 |  0:00:34s\n",
      "epoch 9  | loss: -0.00166| train_accuracy: 0.99446 | valid_accuracy: 0.99631 |  0:00:38s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.99631\n",
      "Best weights from best epoch are automatically used!\n",
      "Optimum Hyperparameters Training [32, 3, 1.5, 0.025]\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.69818 | train_accuracy: 0.63643 | valid_accuracy: 0.63267 |  0:00:05s\n",
      "epoch 1  | loss: 0.04927 | train_accuracy: 0.78786 | valid_accuracy: 0.78865 |  0:00:09s\n",
      "epoch 2  | loss: 0.01578 | train_accuracy: 0.92198 | valid_accuracy: 0.92201 |  0:00:14s\n",
      "epoch 3  | loss: 0.01156 | train_accuracy: 0.95268 | valid_accuracy: 0.95385 |  0:00:19s\n",
      "epoch 4  | loss: 0.00375 | train_accuracy: 0.9626  | valid_accuracy: 0.96354 |  0:00:24s\n",
      "epoch 5  | loss: 0.0059  | train_accuracy: 0.97276 | valid_accuracy: 0.97323 |  0:00:29s\n",
      "epoch 6  | loss: -0.00063| train_accuracy: 0.97345 | valid_accuracy: 0.97647 |  0:00:34s\n",
      "epoch 7  | loss: -0.00102| train_accuracy: 0.97553 | valid_accuracy: 0.97785 |  0:00:39s\n",
      "epoch 8  | loss: -0.00145| train_accuracy: 0.9783  | valid_accuracy: 0.97923 |  0:00:44s\n",
      "epoch 9  | loss: -0.00142| train_accuracy: 0.97992 | valid_accuracy: 0.98016 |  0:00:49s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.98016\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.24758 | train_accuracy: 0.85642 | valid_accuracy: 0.85048 |  0:00:06s\n",
      "epoch 1  | loss: 0.0315  | train_accuracy: 0.95568 | valid_accuracy: 0.95801 |  0:00:12s\n",
      "epoch 2  | loss: 0.01126 | train_accuracy: 0.93029 | valid_accuracy: 0.93493 |  0:00:19s\n",
      "epoch 3  | loss: 0.00913 | train_accuracy: 0.96468 | valid_accuracy: 0.96493 |  0:00:25s\n",
      "epoch 4  | loss: 0.00092 | train_accuracy: 0.9753  | valid_accuracy: 0.97693 |  0:00:31s\n",
      "epoch 5  | loss: -0.00018| train_accuracy: 0.97091 | valid_accuracy: 0.97093 |  0:00:38s\n",
      "epoch 6  | loss: -0.00078| train_accuracy: 0.97091 | valid_accuracy: 0.97093 |  0:00:44s\n",
      "epoch 7  | loss: -0.00083| train_accuracy: 0.98269 | valid_accuracy: 0.98062 |  0:00:50s\n",
      "epoch 8  | loss: 0.00094 | train_accuracy: 0.98823 | valid_accuracy: 0.98708 |  0:00:57s\n",
      "epoch 9  | loss: 0.00048 | train_accuracy: 0.99284 | valid_accuracy: 0.99169 |  0:01:03s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.99169\n",
      "Best weights from best epoch are automatically used!\n",
      "Optimum Hyperparameters Training [32, 5, 1.5, 0.025]\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.24758 | train_accuracy: 0.85642 | valid_accuracy: 0.85048 |  0:00:06s\n",
      "epoch 1  | loss: 0.0315  | train_accuracy: 0.95568 | valid_accuracy: 0.95801 |  0:00:12s\n",
      "epoch 2  | loss: 0.01126 | train_accuracy: 0.93029 | valid_accuracy: 0.93493 |  0:00:19s\n",
      "epoch 3  | loss: 0.00913 | train_accuracy: 0.96468 | valid_accuracy: 0.96493 |  0:00:25s\n",
      "epoch 4  | loss: 0.00092 | train_accuracy: 0.9753  | valid_accuracy: 0.97693 |  0:00:31s\n",
      "epoch 5  | loss: -0.00018| train_accuracy: 0.97091 | valid_accuracy: 0.97093 |  0:00:38s\n",
      "epoch 6  | loss: -0.00078| train_accuracy: 0.97091 | valid_accuracy: 0.97093 |  0:00:44s\n",
      "epoch 7  | loss: -0.00083| train_accuracy: 0.98269 | valid_accuracy: 0.98062 |  0:00:50s\n",
      "epoch 8  | loss: 0.00094 | train_accuracy: 0.98823 | valid_accuracy: 0.98708 |  0:00:57s\n",
      "epoch 9  | loss: 0.00048 | train_accuracy: 0.99284 | valid_accuracy: 0.99169 |  0:01:03s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.99169\n",
      "Best weights from best epoch are automatically used!\n",
      "Optimum Hyperparameters Training [32, 5, 1.0, 0.025]\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.42213 | train_accuracy: 0.87258 | valid_accuracy: 0.86664 |  0:00:06s\n",
      "epoch 1  | loss: 0.07399 | train_accuracy: 0.93998 | valid_accuracy: 0.94416 |  0:00:12s\n",
      "epoch 2  | loss: 0.0164  | train_accuracy: 0.93952 | valid_accuracy: 0.94001 |  0:00:18s\n",
      "epoch 3  | loss: 0.00986 | train_accuracy: 0.9349  | valid_accuracy: 0.93678 |  0:00:24s\n",
      "epoch 4  | loss: 0.01143 | train_accuracy: 0.95452 | valid_accuracy: 0.95431 |  0:00:31s\n",
      "epoch 5  | loss: 0.00514 | train_accuracy: 0.95406 | valid_accuracy: 0.95155 |  0:00:37s\n",
      "epoch 6  | loss: 0.0026  | train_accuracy: 0.95614 | valid_accuracy: 0.95524 |  0:00:43s\n",
      "epoch 7  | loss: -0.00042| train_accuracy: 0.97461 | valid_accuracy: 0.97185 |  0:00:50s\n",
      "epoch 8  | loss: -0.00011| train_accuracy: 0.98546 | valid_accuracy: 0.98616 |  0:00:56s\n",
      "epoch 9  | loss: -0.0012 | train_accuracy: 0.98892 | valid_accuracy: 0.98939 |  0:01:03s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.98939\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.37496 | train_accuracy: 0.82825 | valid_accuracy: 0.81495 |  0:00:06s\n",
      "epoch 1  | loss: 0.0916  | train_accuracy: 0.93006 | valid_accuracy: 0.93355 |  0:00:12s\n",
      "epoch 2  | loss: 0.02926 | train_accuracy: 0.94483 | valid_accuracy: 0.94186 |  0:00:18s\n",
      "epoch 3  | loss: 0.00809 | train_accuracy: 0.96999 | valid_accuracy: 0.95754 |  0:00:25s\n",
      "epoch 4  | loss: 0.00201 | train_accuracy: 0.98361 | valid_accuracy: 0.97831 |  0:00:31s\n",
      "epoch 5  | loss: 0.00106 | train_accuracy: 0.98846 | valid_accuracy: 0.98339 |  0:00:37s\n",
      "epoch 6  | loss: -0.00069| train_accuracy: 0.99331 | valid_accuracy: 0.98985 |  0:00:44s\n",
      "epoch 7  | loss: -0.00052| train_accuracy: 0.99354 | valid_accuracy: 0.99077 |  0:00:50s\n",
      "epoch 8  | loss: -0.00123| train_accuracy: 0.99469 | valid_accuracy: 0.99354 |  0:00:57s\n",
      "epoch 9  | loss: 0.00298 | train_accuracy: 0.99723 | valid_accuracy: 0.99539 |  0:01:03s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.99539\n",
      "Best weights from best epoch are automatically used!\n",
      "Optimum Hyperparameters Training [32, 5, 1.5, 0.025]\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.54181 | train_accuracy: 0.77054 | valid_accuracy: 0.73696 |  0:00:06s\n",
      "epoch 1  | loss: 0.16798 | train_accuracy: 0.87604 | valid_accuracy: 0.86802 |  0:00:12s\n",
      "epoch 2  | loss: 0.11859 | train_accuracy: 0.93075 | valid_accuracy: 0.93401 |  0:00:18s\n",
      "epoch 3  | loss: 0.04312 | train_accuracy: 0.94044 | valid_accuracy: 0.94509 |  0:00:24s\n",
      "epoch 4  | loss: 0.02484 | train_accuracy: 0.95983 | valid_accuracy: 0.96401 |  0:00:31s\n",
      "epoch 5  | loss: 0.0107  | train_accuracy: 0.96745 | valid_accuracy: 0.96954 |  0:00:37s\n",
      "epoch 6  | loss: 0.01419 | train_accuracy: 0.97553 | valid_accuracy: 0.976   |  0:00:43s\n",
      "epoch 7  | loss: 0.01043 | train_accuracy: 0.96122 | valid_accuracy: 0.95847 |  0:00:50s\n",
      "epoch 8  | loss: 0.00855 | train_accuracy: 0.98661 | valid_accuracy: 0.98662 |  0:00:56s\n",
      "epoch 9  | loss: 0.00743 | train_accuracy: 0.98846 | valid_accuracy: 0.98985 |  0:01:02s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.98985\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.37496 | train_accuracy: 0.82825 | valid_accuracy: 0.81495 |  0:00:06s\n",
      "epoch 1  | loss: 0.0916  | train_accuracy: 0.93006 | valid_accuracy: 0.93355 |  0:00:12s\n",
      "epoch 2  | loss: 0.02926 | train_accuracy: 0.94483 | valid_accuracy: 0.94186 |  0:00:18s\n",
      "epoch 3  | loss: 0.00809 | train_accuracy: 0.96999 | valid_accuracy: 0.95754 |  0:00:24s\n",
      "epoch 4  | loss: 0.00201 | train_accuracy: 0.98361 | valid_accuracy: 0.97831 |  0:00:31s\n",
      "epoch 5  | loss: 0.00106 | train_accuracy: 0.98846 | valid_accuracy: 0.98339 |  0:00:37s\n",
      "epoch 6  | loss: -0.00069| train_accuracy: 0.99331 | valid_accuracy: 0.98985 |  0:00:43s\n",
      "epoch 7  | loss: -0.00052| train_accuracy: 0.99354 | valid_accuracy: 0.99077 |  0:00:49s\n",
      "epoch 8  | loss: -0.00123| train_accuracy: 0.99469 | valid_accuracy: 0.99354 |  0:00:56s\n",
      "epoch 9  | loss: 0.00298 | train_accuracy: 0.99723 | valid_accuracy: 0.99539 |  0:01:02s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 0.99539\n",
      "Best weights from best epoch are automatically used!\n",
      "Optimum Hyperparameters Training [32, 5, 1.5, 0.005]\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.24589 | train_accuracy: 0.90051 | valid_accuracy: 0.90448 |  0:00:06s\n",
      "epoch 1  | loss: 0.02148 | train_accuracy: 0.93513 | valid_accuracy: 0.93401 |  0:00:12s\n",
      "epoch 2  | loss: 0.00268 | train_accuracy: 0.98546 | valid_accuracy: 0.98523 |  0:00:18s\n",
      "epoch 3  | loss: 0.00501 | train_accuracy: 0.98684 | valid_accuracy: 0.98754 |  0:00:25s\n",
      "epoch 4  | loss: 0.00029 | train_accuracy: 0.98361 | valid_accuracy: 0.98246 |  0:00:31s\n",
      "epoch 5  | loss: -0.00135| train_accuracy: 0.98684 | valid_accuracy: 0.98477 |  0:00:37s\n",
      "epoch 6  | loss: 0.00404 | train_accuracy: 0.98753 | valid_accuracy: 0.98569 |  0:00:43s\n",
      "epoch 7  | loss: -0.00155| train_accuracy: 0.98823 | valid_accuracy: 0.98708 |  0:00:50s\n",
      "epoch 8  | loss: 0.01028 | train_accuracy: 0.9873  | valid_accuracy: 0.98431 |  0:00:56s\n",
      "epoch 9  | loss: 0.00905 | train_accuracy: 0.98384 | valid_accuracy: 0.98339 |  0:01:02s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 3 and best_valid_accuracy = 0.98754\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.22826 | train_accuracy: 0.89451 | valid_accuracy: 0.89894 |  0:00:06s\n",
      "epoch 1  | loss: 0.03166 | train_accuracy: 0.95337 | valid_accuracy: 0.95847 |  0:00:12s\n",
      "epoch 2  | loss: 0.00942 | train_accuracy: 0.96814 | valid_accuracy: 0.97    |  0:00:18s\n",
      "epoch 3  | loss: 0.01202 | train_accuracy: 0.97576 | valid_accuracy: 0.98062 |  0:00:24s\n",
      "epoch 4  | loss: 6e-05   | train_accuracy: 0.99354 | valid_accuracy: 0.99585 |  0:00:30s\n",
      "epoch 5  | loss: -1e-05  | train_accuracy: 0.99838 | valid_accuracy: 0.99677 |  0:00:37s\n",
      "epoch 6  | loss: -0.00176| train_accuracy: 0.99561 | valid_accuracy: 0.99677 |  0:00:43s\n",
      "epoch 7  | loss: -0.00189| train_accuracy: 0.99608 | valid_accuracy: 0.99723 |  0:00:49s\n",
      "epoch 8  | loss: -0.00188| train_accuracy: 0.99954 | valid_accuracy: 0.99954 |  0:00:55s\n",
      "epoch 9  | loss: -0.00187| train_accuracy: 1.0     | valid_accuracy: 1.0     |  0:01:01s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_accuracy = 1.0\n",
      "Best weights from best epoch are automatically used!\n",
      "Optimum Hyperparameters Training [32, 5, 1.5, 0.02]\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 0.196   | train_accuracy: 0.77054 | valid_accuracy: 0.76511 |  0:00:05s\n",
      "epoch 1  | loss: 0.02538 | train_accuracy: 0.94898 | valid_accuracy: 0.95016 |  0:00:11s\n",
      "epoch 2  | loss: 0.00414 | train_accuracy: 0.99354 | valid_accuracy: 0.99216 |  0:00:18s\n",
      "epoch 3  | loss: 0.00068 | train_accuracy: 0.994   | valid_accuracy: 0.99308 |  0:00:24s\n",
      "epoch 4  | loss: 0.0016  | train_accuracy: 0.99931 | valid_accuracy: 0.99723 |  0:00:30s\n",
      "epoch 5  | loss: 0.01019 | train_accuracy: 0.99261 | valid_accuracy: 0.98662 |  0:00:36s\n",
      "epoch 6  | loss: 0.00287 | train_accuracy: 0.99954 | valid_accuracy: 0.99769 |  0:00:43s\n",
      "epoch 7  | loss: 0.00353 | train_accuracy: 0.99838 | valid_accuracy: 0.99723 |  0:00:49s\n",
      "epoch 8  | loss: 0.0309  | train_accuracy: 0.99908 | valid_accuracy: 0.99769 |  0:00:55s\n",
      "epoch 9  | loss: 0.01745 | train_accuracy: 0.99307 | valid_accuracy: 0.99308 |  0:01:02s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 6 and best_valid_accuracy = 0.99769\n",
      "Best weights from best epoch are automatically used!\n",
      "Optimum Hyperparameters [32, 5, 1.5, 0.02]\n"
     ]
    }
   ],
   "source": [
    "nd_na = [16, 32, 128]\n",
    "n_steps = [3, 4]\n",
    "gammas = [1.0, 1.2, 1.5, 2.0]\n",
    "learn_r = [0.005, 0.01, 0.02, 0.025]\n",
    "\n",
    "opt_ndna = 32\n",
    "opt_nsteps = 3\n",
    "opt_gamma = 1.5\n",
    "opt_lr = 0.025\n",
    "\n",
    "ndna_test_acc = 0\n",
    "for ndna in nd_na:\n",
    "    clf = TabNetClassifier(\n",
    "        n_d=ndna,\n",
    "        n_a=ndna,\n",
    "        n_steps=n_steps[0],\n",
    "        gamma=gammas[0],\n",
    "        lambda_sparse=1.0,\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        optimizer_params=dict(lr=learn_r[0]),\n",
    "        mask_type = 'softmax'\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "        eval_name=['train', 'valid'], batch_size=256,\n",
    "        virtual_batch_size=256,\n",
    "        max_epochs=10, eval_metric=['accuracy']\n",
    "    )\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    test_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "\n",
    "    if test_acc > ndna_test_acc:\n",
    "        opt_ndna = ndna\n",
    "        ndna_test_acc = test_acc\n",
    "        print(\"Optimum Hyperparameters Training\", [opt_ndna, opt_nsteps, opt_gamma, opt_lr])\n",
    "\n",
    "nstep_test_acc = 0\n",
    "for nstep in n_steps:\n",
    "\n",
    "    clf = TabNetClassifier(\n",
    "        n_d=opt_ndna,\n",
    "        n_a=opt_ndna,\n",
    "        n_steps=nstep,\n",
    "        gamma=gammas[0],\n",
    "        lambda_sparse=1.0,\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        optimizer_params=dict(lr=learn_r[0]),\n",
    "        mask_type = 'softmax'\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "        eval_name=['train', 'valid'], batch_size=256,\n",
    "        virtual_batch_size=256,\n",
    "        max_epochs=10, eval_metric=['accuracy']\n",
    "    )\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    test_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "\n",
    "    if test_acc > nstep_test_acc:\n",
    "        opt_nsteps = nstep\n",
    "        nstep_test_acc = test_acc\n",
    "        print(\"Optimum Hyperparameters Training\", [opt_ndna, opt_nsteps, opt_gamma, opt_lr])\n",
    "\n",
    "gams_test_acc = 0\n",
    "for gams in gammas:\n",
    "    clf = TabNetClassifier(\n",
    "        n_d=opt_ndna,\n",
    "        n_a=opt_ndna,\n",
    "        n_steps=opt_nsteps,\n",
    "        gamma=gams,\n",
    "        lambda_sparse=1.0,\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        optimizer_params=dict(lr=learn_r[0]),\n",
    "        mask_type = 'softmax'\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "        eval_name=['train', 'valid'], batch_size=256,\n",
    "        virtual_batch_size=256,\n",
    "        max_epochs=10, eval_metric=['accuracy']\n",
    "    )\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    test_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    if test_acc > gams_test_acc:\n",
    "        opt_gamma = gams\n",
    "        gams_test_acc = test_acc\n",
    "        print(\"Optimum Hyperparameters Training\", [opt_ndna, opt_nsteps, opt_gamma, opt_lr])\n",
    "\n",
    "lr_test_accuracy = 0\n",
    "for lr in learn_r:\n",
    "    clf = TabNetClassifier(\n",
    "        n_d=opt_ndna,\n",
    "        n_a=opt_ndna,\n",
    "        n_steps=opt_nsteps,\n",
    "        gamma=opt_gamma,\n",
    "        lambda_sparse=1.0,\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        optimizer_params=dict(lr=lr),\n",
    "        mask_type = 'softmax'\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "        eval_name=['train', 'valid'], batch_size=256,\n",
    "        virtual_batch_size=256,\n",
    "        max_epochs=10, eval_metric=['accuracy']\n",
    "    )\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    test_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    if test_acc > lr_test_accuracy:\n",
    "        opt_lr = lr\n",
    "        lr_test_accuracy = test_acc\n",
    "        print(\"Optimum Hyperparameters Training\", [opt_ndna, opt_nsteps, opt_gamma, opt_lr])\n",
    "\n",
    "print(\"Optimum Hyperparameters\", [opt_ndna, opt_nsteps, opt_gamma, opt_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_ndna = 32\n",
    "opt_nsteps = 4\n",
    "opt_gamma = 1.0\n",
    "opt_lr = 0.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Sparsity Regularizer $r_M$\n",
    "\n",
    "Note: For an even quicker process, you could directly modify the \"opt_reg_m\" in the \"Optimized Run\" block below and optimize in the range of 0 - 1000000000 w.r.t the accuracy/sparsity (interpretability) tradeoff from the feature mask visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_reg_m(start=0, end=100000000, col_threshold_val=0.20, col_threshold=3, all_mask_pass=None, all_mask_pass_thresh=3, step_size=None, best_reg_m=None, reg_m_acc_dict=None, is_recursive=False):\n",
    "        if reg_m_acc_dict is None:\n",
    "                reg_m_acc_dict = {}\n",
    "\n",
    "        if all_mask_pass == all_mask_pass_thresh:\n",
    "                print(reg_m_acc_dict)\n",
    "                final_reg_m = max(reg_m_acc_dict, key=reg_m_acc_dict.get)\n",
    "                return final_reg_m\n",
    "            \n",
    "        if all_mask_pass is None:\n",
    "                all_mask_pass = 0\n",
    "            \n",
    "        # Fine-tuning around the best found value\n",
    "        best_reg_m = None\n",
    "        break_outer_loop = False\n",
    "\n",
    "        # Determining Magnitude for reg_m\n",
    "        diff = end - start\n",
    "        magnitude = int(math.log10(diff))\n",
    "\n",
    "        reg_m = start\n",
    "        while reg_m <= end and all_mask_pass < all_mask_pass_thresh: #do i need all_mask_pass threshold here?\n",
    "                print(\"reg_m\", reg_m)\n",
    "                if reg_m in reg_m_acc_dict:\n",
    "                    reg_m += step_size\n",
    "                    continue\n",
    "                clf = TabNetClassifier(\n",
    "                    n_d=opt_ndna,\n",
    "                    n_a=opt_ndna,\n",
    "                    n_steps=4,\n",
    "                    gamma=opt_gamma,\n",
    "                    lambda_sparse=1.0,\n",
    "                    cat_idxs=cat_idxs,\n",
    "                    cat_dims=cat_dims,\n",
    "                    optimizer_params=dict(lr=opt_lr),\n",
    "                    mask_type = 'softmax',\n",
    "                    reg_m=reg_m\n",
    "                )\n",
    "                # max epoch 50\n",
    "                clf.fit(\n",
    "                    X_train=X_train, y_train=y_train,\n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                    eval_name=['train', 'valid'],\n",
    "                    max_epochs=30, eval_metric=['accuracy']\n",
    "                )\n",
    "\n",
    "                y_pred = clf.predict(X_test)\n",
    "                test_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "\n",
    "                print(f\"FINAL TEST SCORE FOR {dataset} : {test_acc}\")\n",
    "\n",
    "                explain_matrix, masks = clf.explain(X_test)\n",
    "\n",
    "                # Extract the first 50 samples from each matrix\n",
    "                masks_dict = {}\n",
    "                for key, value in masks.items():\n",
    "                    masks_dict[key] = value[:50]\n",
    "\n",
    "                # Normalize each extracted matrix so that its sum is 1\n",
    "                for key, value in masks_dict.items():\n",
    "                    total_sum = value.sum()\n",
    "                    \n",
    "                    # Avoid division by zero\n",
    "                    if total_sum == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    masks_dict[key] = value / total_sum\n",
    "\n",
    "                mask_threshold = opt_nsteps // 2 + 1\n",
    "                mask_pass_count = 0\n",
    "\n",
    "                for key, value in masks_dict.items():\n",
    "                    column_sums = value.sum(axis=0)\n",
    "                    # print(f\"Sum of columns for matrix with key {key}: {column_sums}\")\n",
    "\n",
    "                    # Check which columns are greater than col_threshold_val\n",
    "                    cols_above_threshold = [i for i, col_sum in enumerate(column_sums) if col_sum > col_threshold_val]\n",
    "                    print(f\"Columns in matrix with key {key} that are greater than the threshold value: {cols_above_threshold}\")\n",
    "\n",
    "                    if col_threshold-1 <= len(cols_above_threshold) <= col_threshold+1:\n",
    "                        mask_pass_count += 1\n",
    "                        print(\"Num Mask Pass Threshold:\", mask_pass_count)\n",
    "                    if mask_pass_count >= mask_threshold:\n",
    "                        if len(reg_m_acc_dict) == 0:\n",
    "                            all_mask_pass += 1\n",
    "                            best_reg_m = reg_m\n",
    "                            reg_m_acc_dict[reg_m] = test_acc\n",
    "                            break_outer_loop = True\n",
    "                            break\n",
    "                        elif test_acc > max(reg_m_acc_dict.values()):\n",
    "                            all_mask_pass += 1\n",
    "                            reg_m_acc_dict[reg_m] = test_acc\n",
    "                            best_reg_m = reg_m\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Lesser Acc, Break\")\n",
    "                            break\n",
    "                    \n",
    "                if break_outer_loop:\n",
    "                    break\n",
    "\n",
    "                if is_recursive:\n",
    "                    reg_m += step_size\n",
    "                elif reg_m == 0:\n",
    "                    reg_m = 10\n",
    "                else:\n",
    "                    reg_m *= 10\n",
    "\n",
    "\n",
    "        # Check conditions after looping over all possible reg_m values\n",
    "        if best_reg_m is not None and len(reg_m_acc_dict) == 1: # i need to add the condition where i hit all mask pass and return the funct directly\n",
    "                print('Breaked')\n",
    "                magnitude = math.floor(math.log10(best_reg_m))\n",
    "                if magnitude >= 1:\n",
    "                    step_size = 10**(magnitude-1)\n",
    "                else:\n",
    "                    step_size = 10**(magnitude)\n",
    "                # Recursively refine the search with updated boundaries and reduced depth\n",
    "                new_start = int(max(start, best_reg_m - step_size))\n",
    "                new_end = int(min(end, best_reg_m + step_size))\n",
    "                return search_best_reg_m(new_start, new_end, col_threshold, col_threshold_val, all_mask_pass, all_mask_pass_thresh, step_size, best_reg_m, reg_m_acc_dict, is_recursive=True)\n",
    "        elif len(reg_m_acc_dict)==0:\n",
    "                return \"Did not pass! Increase start/end Search Range!\"\n",
    "        else:\n",
    "                final_reg_m = max(reg_m_acc_dict, key=reg_m_acc_dict.get)\n",
    "                return final_reg_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_m 0\n",
      "Device used cuda: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: -1.08538| train_accuracy: 0.78576 | valid_accuracy: 0.77255 |  0:00:10s\n",
      "epoch 1  | loss: -1.3059 | train_accuracy: 0.8347  | valid_accuracy: 0.82172 |  0:00:20s\n",
      "epoch 2  | loss: -1.34094| train_accuracy: 0.84011 | valid_accuracy: 0.82866 |  0:00:30s\n",
      "epoch 3  | loss: -1.35046| train_accuracy: 0.83732 | valid_accuracy: 0.82986 |  0:00:40s\n",
      "epoch 4  | loss: -1.36552| train_accuracy: 0.8274  | valid_accuracy: 0.82262 |  0:00:50s\n",
      "epoch 5  | loss: -1.37592| train_accuracy: 0.82539 | valid_accuracy: 0.82293 |  0:01:00s\n",
      "epoch 6  | loss: -1.38363| train_accuracy: 0.81311 | valid_accuracy: 0.80784 |  0:01:10s\n",
      "epoch 7  | loss: -1.3882 | train_accuracy: 0.81701 | valid_accuracy: 0.81388 |  0:01:20s\n",
      "epoch 8  | loss: -1.39216| train_accuracy: 0.83223 | valid_accuracy: 0.82624 |  0:01:30s\n",
      "epoch 9  | loss: -1.39386| train_accuracy: 0.85698 | valid_accuracy: 0.84796 |  0:01:40s\n",
      "epoch 10 | loss: -1.39963| train_accuracy: 0.81709 | valid_accuracy: 0.81026 |  0:01:50s\n",
      "epoch 11 | loss: -1.39906| train_accuracy: 0.85803 | valid_accuracy: 0.85189 |  0:02:00s\n",
      "epoch 12 | loss: -1.40427| train_accuracy: 0.86502 | valid_accuracy: 0.84827 |  0:02:10s\n",
      "epoch 13 | loss: -1.40613| train_accuracy: 0.87262 | valid_accuracy: 0.85701 |  0:02:20s\n",
      "epoch 14 | loss: -1.40371| train_accuracy: 0.87672 | valid_accuracy: 0.86425 |  0:02:30s\n",
      "epoch 15 | loss: -1.41001| train_accuracy: 0.87436 | valid_accuracy: 0.8549  |  0:02:40s\n",
      "epoch 16 | loss: -1.4068 | train_accuracy: 0.88039 | valid_accuracy: 0.85792 |  0:02:50s\n",
      "epoch 17 | loss: -1.40882| train_accuracy: 0.88031 | valid_accuracy: 0.86154 |  0:03:00s\n",
      "epoch 18 | loss: -1.41292| train_accuracy: 0.87741 | valid_accuracy: 0.85792 |  0:03:10s\n",
      "epoch 19 | loss: -1.41522| train_accuracy: 0.88182 | valid_accuracy: 0.86395 |  0:03:20s\n",
      "epoch 20 | loss: -1.41309| train_accuracy: 0.87954 | valid_accuracy: 0.85611 |  0:03:30s\n",
      "epoch 21 | loss: -1.4098 | train_accuracy: 0.88124 | valid_accuracy: 0.85913 |  0:03:40s\n",
      "epoch 22 | loss: -1.40953| train_accuracy: 0.87965 | valid_accuracy: 0.85852 |  0:03:50s\n",
      "epoch 23 | loss: -1.41439| train_accuracy: 0.87305 | valid_accuracy: 0.85249 |  0:04:00s\n",
      "epoch 24 | loss: -1.41499| train_accuracy: 0.88294 | valid_accuracy: 0.85852 |  0:04:11s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_valid_accuracy = 0.86425\n",
      "Best weights from best epoch are automatically used!\n",
      "FINAL TEST SCORE FOR census-income : 0.8681777512675216\n",
      "Columns in matrix with key 0 that are greater than the threshold value: []\n",
      "Columns in matrix with key 1 that are greater than the threshold value: []\n",
      "Columns in matrix with key 2 that are greater than the threshold value: [8]\n",
      "Columns in matrix with key 3 that are greater than the threshold value: []\n",
      "reg_m 10\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 46.26225| train_accuracy: 0.79986 | valid_accuracy: 0.78643 |  0:00:10s\n",
      "epoch 1  | loss: 46.24525| train_accuracy: 0.83338 | valid_accuracy: 0.8187  |  0:00:20s\n",
      "epoch 2  | loss: 46.18337| train_accuracy: 0.84173 | valid_accuracy: 0.83107 |  0:00:30s\n",
      "epoch 3  | loss: 45.98407| train_accuracy: 0.84864 | valid_accuracy: 0.84103 |  0:00:40s\n",
      "epoch 4  | loss: 46.0819 | train_accuracy: 0.83926 | valid_accuracy: 0.83439 |  0:00:50s\n",
      "epoch 5  | loss: 45.97265| train_accuracy: 0.85096 | valid_accuracy: 0.83801 |  0:01:00s\n",
      "epoch 6  | loss: 45.82125| train_accuracy: 0.85501 | valid_accuracy: 0.84193 |  0:01:10s\n",
      "epoch 7  | loss: 45.74163| train_accuracy: 0.80805 | valid_accuracy: 0.80452 |  0:01:20s\n",
      "epoch 8  | loss: 45.70577| train_accuracy: 0.85397 | valid_accuracy: 0.84495 |  0:01:30s\n",
      "epoch 9  | loss: 45.62623| train_accuracy: 0.85903 | valid_accuracy: 0.84374 |  0:01:40s\n",
      "epoch 10 | loss: 45.67615| train_accuracy: 0.80183 | valid_accuracy: 0.78643 |  0:01:50s\n",
      "epoch 11 | loss: 45.33144| train_accuracy: 0.84787 | valid_accuracy: 0.83741 |  0:02:00s\n",
      "epoch 12 | loss: 45.53242| train_accuracy: 0.82883 | valid_accuracy: 0.81357 |  0:02:10s\n",
      "epoch 13 | loss: 45.47169| train_accuracy: 0.8549  | valid_accuracy: 0.84706 |  0:02:21s\n",
      "epoch 14 | loss: 45.45899| train_accuracy: 0.85208 | valid_accuracy: 0.83469 |  0:02:31s\n",
      "epoch 15 | loss: 45.57047| train_accuracy: 0.8564  | valid_accuracy: 0.84193 |  0:02:41s\n",
      "epoch 16 | loss: 45.35158| train_accuracy: 0.83412 | valid_accuracy: 0.83017 |  0:02:51s\n",
      "epoch 17 | loss: 45.27814| train_accuracy: 0.85528 | valid_accuracy: 0.84857 |  0:03:01s\n",
      "epoch 18 | loss: 45.23931| train_accuracy: 0.83041 | valid_accuracy: 0.81237 |  0:03:11s\n",
      "epoch 19 | loss: 45.39234| train_accuracy: 0.83207 | valid_accuracy: 0.81357 |  0:03:21s\n",
      "epoch 20 | loss: 45.35673| train_accuracy: 0.86969 | valid_accuracy: 0.85641 |  0:03:31s\n",
      "epoch 21 | loss: 45.32696| train_accuracy: 0.86251 | valid_accuracy: 0.84706 |  0:03:41s\n",
      "epoch 22 | loss: 45.47489| train_accuracy: 0.86046 | valid_accuracy: 0.84585 |  0:03:51s\n",
      "epoch 23 | loss: 45.39208| train_accuracy: 0.8678  | valid_accuracy: 0.85611 |  0:04:01s\n",
      "epoch 24 | loss: 45.28241| train_accuracy: 0.86637 | valid_accuracy: 0.8543  |  0:04:11s\n",
      "epoch 25 | loss: 45.36922| train_accuracy: 0.84014 | valid_accuracy: 0.82775 |  0:04:20s\n",
      "epoch 26 | loss: 45.27599| train_accuracy: 0.86969 | valid_accuracy: 0.85249 |  0:04:31s\n",
      "epoch 27 | loss: 45.34192| train_accuracy: 0.85652 | valid_accuracy: 0.84857 |  0:04:41s\n",
      "epoch 28 | loss: 45.26961| train_accuracy: 0.84679 | valid_accuracy: 0.8362  |  0:04:51s\n",
      "epoch 29 | loss: 45.35541| train_accuracy: 0.83134 | valid_accuracy: 0.82504 |  0:05:01s\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 20 and best_valid_accuracy = 0.85641\n",
      "Best weights from best epoch are automatically used!\n",
      "FINAL TEST SCORE FOR census-income : 0.8660900685952878\n",
      "Columns in matrix with key 0 that are greater than the threshold value: [0, 5]\n",
      "Num Mask Pass Threshold: 1\n",
      "Columns in matrix with key 1 that are greater than the threshold value: [0, 11]\n",
      "Num Mask Pass Threshold: 2\n",
      "Columns in matrix with key 2 that are greater than the threshold value: [1]\n",
      "Columns in matrix with key 3 that are greater than the threshold value: [0, 5]\n",
      "Num Mask Pass Threshold: 3\n",
      "Breaked\n",
      "reg_m 9\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 41.52055| train_accuracy: 0.7863  | valid_accuracy: 0.76802 |  0:00:10s\n",
      "epoch 1  | loss: 41.46963| train_accuracy: 0.83786 | valid_accuracy: 0.82443 |  0:00:20s\n",
      "epoch 2  | loss: 41.38733| train_accuracy: 0.8474  | valid_accuracy: 0.8365  |  0:00:30s\n",
      "epoch 3  | loss: 41.19034| train_accuracy: 0.84586 | valid_accuracy: 0.83409 |  0:00:40s\n",
      "epoch 4  | loss: 41.28685| train_accuracy: 0.8296  | valid_accuracy: 0.82353 |  0:00:51s\n",
      "epoch 5  | loss: 41.19909| train_accuracy: 0.83153 | valid_accuracy: 0.82504 |  0:01:01s\n",
      "epoch 6  | loss: 41.02058| train_accuracy: 0.84532 | valid_accuracy: 0.83167 |  0:01:11s\n",
      "epoch 7  | loss: 40.91159| train_accuracy: 0.74486 | valid_accuracy: 0.746   |  0:01:21s\n",
      "epoch 8  | loss: 40.91569| train_accuracy: 0.73266 | valid_accuracy: 0.727   |  0:01:32s\n",
      "epoch 9  | loss: 40.82945| train_accuracy: 0.83617 | valid_accuracy: 0.81991 |  0:01:42s\n",
      "epoch 10 | loss: 40.88273| train_accuracy: 0.81149 | valid_accuracy: 0.80422 |  0:01:52s\n",
      "epoch 11 | loss: 40.54493| train_accuracy: 0.85088 | valid_accuracy: 0.84615 |  0:02:02s\n",
      "epoch 12 | loss: 40.80038| train_accuracy: 0.83362 | valid_accuracy: 0.81569 |  0:02:13s\n",
      "epoch 13 | loss: 40.7335 | train_accuracy: 0.86397 | valid_accuracy: 0.85581 |  0:02:23s\n",
      "epoch 14 | loss: 40.74854| train_accuracy: 0.83277 | valid_accuracy: 0.81629 |  0:02:33s\n",
      "epoch 15 | loss: 40.8412 | train_accuracy: 0.86617 | valid_accuracy: 0.854   |  0:02:43s\n",
      "epoch 16 | loss: 40.66339| train_accuracy: 0.83064 | valid_accuracy: 0.81176 |  0:02:53s\n",
      "epoch 17 | loss: 40.68131| train_accuracy: 0.85412 | valid_accuracy: 0.8365  |  0:03:03s\n",
      "epoch 18 | loss: 40.60376| train_accuracy: 0.86061 | valid_accuracy: 0.84495 |  0:03:13s\n",
      "epoch 19 | loss: 40.73533| train_accuracy: 0.85741 | valid_accuracy: 0.84404 |  0:03:23s\n",
      "epoch 20 | loss: 40.69353| train_accuracy: 0.85756 | valid_accuracy: 0.85158 |  0:03:33s\n",
      "epoch 21 | loss: 40.64128| train_accuracy: 0.84856 | valid_accuracy: 0.8359  |  0:03:44s\n",
      "epoch 22 | loss: 40.76078| train_accuracy: 0.8671  | valid_accuracy: 0.84947 |  0:03:54s\n",
      "epoch 23 | loss: 40.7038 | train_accuracy: 0.85536 | valid_accuracy: 0.84344 |  0:04:04s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_valid_accuracy = 0.85581\n",
      "Best weights from best epoch are automatically used!\n",
      "FINAL TEST SCORE FOR census-income : 0.8651953474500448\n",
      "Columns in matrix with key 0 that are greater than the threshold value: []\n",
      "Num Mask Pass Threshold: 1\n",
      "Columns in matrix with key 1 that are greater than the threshold value: []\n",
      "Num Mask Pass Threshold: 2\n",
      "Columns in matrix with key 2 that are greater than the threshold value: []\n",
      "Num Mask Pass Threshold: 3\n",
      "Lesser Acc, Break\n",
      "reg_m 10\n",
      "reg_m 11\n",
      "Device used cuda: 0\n",
      "epoch 0  | loss: 50.99341| train_accuracy: 0.79368 | valid_accuracy: 0.77798 |  0:00:10s\n",
      "epoch 1  | loss: 50.99424| train_accuracy: 0.83068 | valid_accuracy: 0.81418 |  0:00:20s\n",
      "epoch 2  | loss: 50.9327 | train_accuracy: 0.84134 | valid_accuracy: 0.82745 |  0:00:30s\n",
      "epoch 3  | loss: 50.6824 | train_accuracy: 0.84891 | valid_accuracy: 0.83379 |  0:00:40s\n",
      "epoch 4  | loss: 50.78636| train_accuracy: 0.85277 | valid_accuracy: 0.84253 |  0:00:50s\n",
      "epoch 5  | loss: 50.62949| train_accuracy: 0.85521 | valid_accuracy: 0.83922 |  0:01:01s\n",
      "epoch 6  | loss: 50.42269| train_accuracy: 0.81326 | valid_accuracy: 0.79789 |  0:01:11s\n",
      "epoch 7  | loss: 50.35877| train_accuracy: 0.82064 | valid_accuracy: 0.80935 |  0:01:21s\n",
      "epoch 8  | loss: 50.3922 | train_accuracy: 0.83161 | valid_accuracy: 0.8187  |  0:01:31s\n",
      "epoch 9  | loss: 50.30642| train_accuracy: 0.81589 | valid_accuracy: 0.8009  |  0:01:41s\n",
      "epoch 10 | loss: 50.39396| train_accuracy: 0.81465 | valid_accuracy: 0.80965 |  0:01:51s\n",
      "epoch 11 | loss: 49.96878| train_accuracy: 0.82473 | valid_accuracy: 0.80814 |  0:02:02s\n",
      "epoch 12 | loss: 50.16653| train_accuracy: 0.81203 | valid_accuracy: 0.8006  |  0:02:12s\n",
      "epoch 13 | loss: 50.10351| train_accuracy: 0.85779 | valid_accuracy: 0.84495 |  0:02:22s\n",
      "epoch 14 | loss: 50.12702| train_accuracy: 0.84833 | valid_accuracy: 0.8356  |  0:02:32s\n",
      "epoch 15 | loss: 50.23312| train_accuracy: 0.83466 | valid_accuracy: 0.8184  |  0:02:42s\n",
      "epoch 16 | loss: 50.00494| train_accuracy: 0.83443 | valid_accuracy: 0.82655 |  0:02:52s\n",
      "epoch 17 | loss: 50.01544| train_accuracy: 0.84408 | valid_accuracy: 0.82685 |  0:03:02s\n",
      "epoch 18 | loss: 49.87303| train_accuracy: 0.82929 | valid_accuracy: 0.80995 |  0:03:13s\n",
      "epoch 19 | loss: 50.06042| train_accuracy: 0.85636 | valid_accuracy: 0.83982 |  0:03:23s\n",
      "epoch 20 | loss: 50.01332| train_accuracy: 0.84547 | valid_accuracy: 0.82986 |  0:03:33s\n",
      "epoch 21 | loss: 49.98837| train_accuracy: 0.83516 | valid_accuracy: 0.81719 |  0:03:43s\n",
      "epoch 22 | loss: 50.15839| train_accuracy: 0.85721 | valid_accuracy: 0.83831 |  0:03:54s\n",
      "epoch 23 | loss: 50.06829| train_accuracy: 0.81755 | valid_accuracy: 0.80724 |  0:04:04s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_valid_accuracy = 0.84495\n",
      "Best weights from best epoch are automatically used!\n",
      "FINAL TEST SCORE FOR census-income : 0.8541604533253803\n",
      "Columns in matrix with key 0 that are greater than the threshold value: []\n",
      "Num Mask Pass Threshold: 1\n",
      "Columns in matrix with key 1 that are greater than the threshold value: []\n",
      "Num Mask Pass Threshold: 2\n",
      "Columns in matrix with key 2 that are greater than the threshold value: []\n",
      "Num Mask Pass Threshold: 3\n",
      "Lesser Acc, Break\n",
      "opt_reg_m for Best Mask 10\n"
     ]
    }
   ],
   "source": [
    "opt_reg_m = search_best_reg_m()\n",
    "print(\"opt_reg_m for Best Mask\", opt_reg_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_reg_m = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 4 1.0 0.025 10\n"
     ]
    }
   ],
   "source": [
    "print(opt_ndna, opt_nsteps, opt_gamma, opt_lr, opt_reg_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used cuda: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 41.52055| train_accuracy: 0.7863  | valid_accuracy: 0.76802 |  0:00:10s\n",
      "epoch 1  | loss: 41.46963| train_accuracy: 0.83786 | valid_accuracy: 0.82443 |  0:00:20s\n",
      "epoch 2  | loss: 41.38733| train_accuracy: 0.8474  | valid_accuracy: 0.8365  |  0:00:30s\n",
      "epoch 3  | loss: 41.19034| train_accuracy: 0.84586 | valid_accuracy: 0.83409 |  0:00:40s\n",
      "epoch 4  | loss: 41.28685| train_accuracy: 0.8296  | valid_accuracy: 0.82353 |  0:00:50s\n",
      "epoch 5  | loss: 41.19909| train_accuracy: 0.83153 | valid_accuracy: 0.82504 |  0:01:00s\n",
      "epoch 6  | loss: 41.02058| train_accuracy: 0.84532 | valid_accuracy: 0.83167 |  0:01:10s\n",
      "epoch 7  | loss: 40.91159| train_accuracy: 0.74486 | valid_accuracy: 0.746   |  0:01:20s\n",
      "epoch 8  | loss: 40.91569| train_accuracy: 0.73266 | valid_accuracy: 0.727   |  0:01:30s\n",
      "epoch 9  | loss: 40.82945| train_accuracy: 0.83617 | valid_accuracy: 0.81991 |  0:01:40s\n",
      "epoch 10 | loss: 40.88273| train_accuracy: 0.81149 | valid_accuracy: 0.80422 |  0:01:50s\n",
      "epoch 11 | loss: 40.54493| train_accuracy: 0.85088 | valid_accuracy: 0.84615 |  0:02:00s\n",
      "epoch 12 | loss: 40.80038| train_accuracy: 0.83362 | valid_accuracy: 0.81569 |  0:02:10s\n",
      "epoch 13 | loss: 40.7335 | train_accuracy: 0.86397 | valid_accuracy: 0.85581 |  0:02:20s\n",
      "epoch 14 | loss: 40.74854| train_accuracy: 0.83277 | valid_accuracy: 0.81629 |  0:02:30s\n",
      "epoch 15 | loss: 40.8412 | train_accuracy: 0.86617 | valid_accuracy: 0.854   |  0:02:40s\n",
      "epoch 16 | loss: 40.66339| train_accuracy: 0.83064 | valid_accuracy: 0.81176 |  0:02:50s\n",
      "epoch 17 | loss: 40.68131| train_accuracy: 0.85412 | valid_accuracy: 0.8365  |  0:03:00s\n",
      "epoch 18 | loss: 40.60376| train_accuracy: 0.86061 | valid_accuracy: 0.84495 |  0:03:10s\n",
      "epoch 19 | loss: 40.73533| train_accuracy: 0.85741 | valid_accuracy: 0.84404 |  0:03:19s\n",
      "epoch 20 | loss: 40.69353| train_accuracy: 0.85756 | valid_accuracy: 0.85158 |  0:03:29s\n",
      "epoch 21 | loss: 40.64128| train_accuracy: 0.84856 | valid_accuracy: 0.8359  |  0:03:39s\n",
      "epoch 22 | loss: 40.76078| train_accuracy: 0.8671  | valid_accuracy: 0.84947 |  0:03:49s\n",
      "epoch 23 | loss: 40.7038 | train_accuracy: 0.85536 | valid_accuracy: 0.84344 |  0:03:59s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_valid_accuracy = 0.85581\n",
      "Best weights from best epoch are automatically used!\n",
      "FINAL TEST SCORE FOR census-income : 0.8651953474500448\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = TabNetClassifier(\n",
    "    n_d=opt_ndna,\n",
    "    n_a=opt_ndna,\n",
    "    n_steps=opt_nsteps,\n",
    "    gamma=opt_gamma,\n",
    "    lambda_sparse=1.0,\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    optimizer_params=dict(lr=opt_lr),\n",
    "    mask_type = 'softmax',\n",
    "    reg_m=opt_reg_m\n",
    ")\n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    max_epochs=100, eval_metric=['accuracy']\n",
    ")\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "\n",
    "print(f\"FINAL TEST SCORE FOR {dataset} : {test_acc}\")\n",
    "\n",
    "explain_matrix, masks = clf.explain(X_test)\n",
    "fig, axs = plt.subplots(1, opt_nsteps, figsize=(20, 20))\n",
    "for i in range(opt_nsteps):\n",
    "    axs[i].imshow(masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")\n",
    "    axs[i].set_ylabel(\"Test Samples\")\n",
    "    axs[i].set_xlabel(\"Features\")\n",
    "plt.savefig(f\"{dataset}_feature_mask_kld_{opt_reg_m}_accuracy_{test_acc}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpretabnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
